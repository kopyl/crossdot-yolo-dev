{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import urllib.request\n",
    "import cv2\n",
    "import numpy as np\n",
    "import ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = YOLO(\"best.pt\")  # load a pretrained model (recommended for training)\n",
    "model = YOLO(\"yolov8_custom_trained_model.pt\")  # load a pretrained model (recommended for training)\n",
    "class_names = {0: 'adult', 1: 'nipple', 2: 'underage'}\n",
    "\n",
    "# model.export(format=\"onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Found https://cdn.discordapp.com/attachments/1096822099345145969/1118670993729257603/18c7b730-0548-11ee-84fb-49b3df39f0f4_0.jpeg locally at 18c7b730-0548-11ee-84fb-49b3df39f0f4_0.jpeg\n",
      "image 1/1 /Users/olehkopyl/Dropbox/Development/Clients/Kilim Choi/Projects/nsfw-classifier/yolov8/18c7b730-0548-11ee-84fb-49b3df39f0f4_0.jpeg: 800x512 1 adult, 2 nipples, 978.8ms\n",
      "Speed: 8.4ms preprocess, 978.8ms inference, 1.5ms postprocess per image at shape (1, 3, 800, 800)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['adult', 0.8801199197769165],\n",
       " ['nipple', 0.5891961455345154],\n",
       " ['nipple', 0.5870662331581116]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_image_from_url(image_url):\n",
    "    try:\n",
    "        req = urllib.request.Request(image_url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        with urllib.request.urlopen(req) as url:\n",
    "            image_contents = url.read()\n",
    "    except urllib.error.HTTPError as e:\n",
    "        return {\"error\": str(e)}\n",
    "    except ConnectionResetError:\n",
    "        return {\"error\": \"ConnectionResetError\"}\n",
    "\n",
    "    arr = np.asarray(bytearray(image_contents), dtype=np.uint8)\n",
    "    img = cv2.imdecode(arr, -1)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img\n",
    "\n",
    "\n",
    "def predict_single_image(image_url):\n",
    "    # image = get_image_from_url(image_url)\n",
    "    results = model(image_url)\n",
    "    processed_resutls = []\n",
    "\n",
    "    for n, result in enumerate(results):\n",
    "        boxes = result.boxes  # Boxes object for bbox outputs\n",
    "        for box in boxes:\n",
    "            confidence = box.conf.item()\n",
    "            name = class_names[int(box.cls.item())]\n",
    "            processed_resutls.append([name, confidence])\n",
    "\n",
    "    return processed_resutls\n",
    "\n",
    "\n",
    "# predict_single_image(\"https://cdn.discordapp.com/attachments/1083610420096532581/1116549067036708894/2764483352_A_RAW_photo_of_a_13_year_old_white_girl_blonde_ha_xl-beta-v2-2-2.png\")\n",
    "predict_single_image(\"https://cdn.discordapp.com/attachments/1096822099345145969/1118670993729257603/18c7b730-0548-11ee-84fb-49b3df39f0f4_0.jpeg\")\n",
    "# predict_single_image(\"https://cdn.discordapp.com/attachments/1083610420096532581/1116549067036708894/2764483352_A_RAW_photo_of_a_13_year_old_white_girl_blonde_ha_xl-beta-v2-2-2.png\")\n",
    "# predict_single_image(\"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/fb2c6c74-97ee-4cda-fda4-8ea23f892200/width=707/8651cacc-5138-4053-e11a-a170817ffd00.jpeg\")\n",
    "# predict_single_image(\"https://i.imgur.com/NWuGqwl.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify', or 'pose'.\n",
      "Loading yolov8_custom_trained_model_c.onnx for ONNX Runtime inference...\n",
      "\n"
     ]
    },
    {
     "ename": "InvalidArgument",
     "evalue": "[ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Got invalid dimensions for input: images for the following indices\n index: 2 Got: 640 Expected: 800\n index: 3 Got: 640 Expected: 800\n Please fix either the inputs or the model.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m image \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39mresize((\u001b[39m800\u001b[39m, \u001b[39m800\u001b[39m))\n\u001b[1;32m      8\u001b[0m model_onnx \u001b[39m=\u001b[39m YOLO(\u001b[39m\"\u001b[39m\u001b[39myolov8_custom_trained_model_c.onnx\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m results \u001b[39m=\u001b[39m model_onnx\u001b[39m.\u001b[39;49mpredict(image)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.13/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.13/lib/python3.8/site-packages/ultralytics/yolo/engine/model.py:253\u001b[0m, in \u001b[0;36mYOLO.predict\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# only update args if predictor is already setup\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor\u001b[39m.\u001b[39margs \u001b[39m=\u001b[39m get_cfg(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor\u001b[39m.\u001b[39margs, overrides)\n\u001b[0;32m--> 253\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor\u001b[39m.\u001b[39mpredict_cli(source\u001b[39m=\u001b[39msource) \u001b[39mif\u001b[39;00m is_cli \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredictor(source\u001b[39m=\u001b[39;49msource, stream\u001b[39m=\u001b[39;49mstream)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.13/lib/python3.8/site-packages/ultralytics/yolo/engine/predictor.py:184\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream)\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream_inference(source, model)\n\u001b[1;32m    183\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 184\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream_inference(source, model))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.13/lib/python3.8/site-packages/torch/autograd/grad_mode.py:43\u001b[0m, in \u001b[0;36m_DecoratorContextManager._wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m     \u001b[39m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 43\u001b[0m         response \u001b[39m=\u001b[39m gen\u001b[39m.\u001b[39;49msend(\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m     45\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m             \u001b[39m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.13/lib/python3.8/site-packages/ultralytics/yolo/engine/predictor.py:240\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[39m# Inference\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[39mwith\u001b[39;00m profilers[\u001b[39m1\u001b[39m]:\n\u001b[0;32m--> 240\u001b[0m     preds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(im, augment\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49maugment, visualize\u001b[39m=\u001b[39;49mvisualize)\n\u001b[1;32m    242\u001b[0m \u001b[39m# Postprocess\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[39mwith\u001b[39;00m profilers[\u001b[39m2\u001b[39m]:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.13/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.13/lib/python3.8/site-packages/ultralytics/nn/autobackend.py:323\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[0;34m(self, im, augment, visualize)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39monnx:  \u001b[39m# ONNX Runtime\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     im \u001b[39m=\u001b[39m im\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()  \u001b[39m# torch to numpy\u001b[39;00m\n\u001b[0;32m--> 323\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_names, {\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mget_inputs()[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mname: im})\n\u001b[1;32m    324\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mxml:  \u001b[39m# OpenVINO\u001b[39;00m\n\u001b[1;32m    325\u001b[0m     im \u001b[39m=\u001b[39m im\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()  \u001b[39m# FP32\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.13/lib/python3.8/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:200\u001b[0m, in \u001b[0;36mSession.run\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    198\u001b[0m     output_names \u001b[39m=\u001b[39m [output\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs_meta]\n\u001b[1;32m    199\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sess\u001b[39m.\u001b[39;49mrun(output_names, input_feed, run_options)\n\u001b[1;32m    201\u001b[0m \u001b[39mexcept\u001b[39;00m C\u001b[39m.\u001b[39mEPFail \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    202\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_fallback:\n",
      "\u001b[0;31mInvalidArgument\u001b[0m: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Got invalid dimensions for input: images for the following indices\n index: 2 Got: 640 Expected: 800\n index: 3 Got: 640 Expected: 800\n Please fix either the inputs or the model."
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from ultralytics import YOLO\n",
    "\n",
    "image = Image.open(requests.get(\"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/fb2c6c74-97ee-4cda-fda4-8ea23f892200/width=707/8651cacc-5138-4053-e11a-a170817ffd00.jpeg\", stream=True).raw).convert(\"RGB\")\n",
    "image = image.resize((800, 800))\n",
    "\n",
    "model_onnx = YOLO(\"yolov8_custom_trained_model_c.onnx\")\n",
    "results = model_onnx.predict(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 800\n"
     ]
    }
   ],
   "source": [
    "# get image dimensions\n",
    "width, height = image.size\n",
    "\n",
    "print(width, height)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
